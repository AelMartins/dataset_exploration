Analise Weka Multilayer Perceptron

- Introduzir o objetivo deste documento
- Apresentar o dataset utilizado para a analise
- Explicar o que é o algoritimo de rede neural Multilayer Perceptron (MLP)
- Realizar as etapas de treinamento
    - Com o auxilio dos materiais de aula, selecionar os atributos e as classes utilizando as formulas de calculo da quantidade de neuronios
    - Apresentar uma tabela de resultados para cada camada
    - Escolher a melhor opção de camada, Learning Rate e Training Time

- Concluir a analise

- Adicionar as apendices de cada retorno de resposta do programa

***

nome do arquivo - ANALISE_WEKA_MULTILAYER_PERCEPTRON.pdf
titulo - Análise do algoritmo de rede neural Multilayer Perceptron no WEKA

a introdução do objetivo do documento - Em resumo é utilizar o programa WEKA que disponibiliza o algoritmo MLP para treinar o dataset mencionado mais a frente com o objetivo de extrair um melhor resultado de respostas enquanto estudamos cada passo do processo.
apresentação do dataset - Pode ser a mesma do ultimo trabalho

o que é o algoritimo MLP?

O algoritmo de rede neural Multilayer Perceptron (MLP) é um tipo de rede neural feedforward composta por várias camadas de neurônios. Cada neurônio em uma camada está conectado a todos os neurônios da camada seguinte, o que permite a modelagem de relações complexas nos dados.

Principais componentes e características do MLP:

1. **Camadas**: O MLP é composto por uma camada de entrada, uma ou mais camadas ocultas e uma camada de saída. A camada de entrada recebe os dados, as camadas ocultas realizam cálculos e a camada de saída produz a resposta.

2. **Ativação**: Cada neurônio aplica uma função de ativação a uma combinação ponderada das entradas. Funções comuns incluem a sigmoid, ReLU (Rectified Linear Unit) e tanh. Essas funções introduzem não-linearidades no modelo.

3. **Propagação para frente**: Durante a fase de treinamento, os dados de entrada são passados através da rede, camada por camada, até chegar à camada de saída, onde são geradas as previsões.

4. **Retropropagação**: Após a propagação para frente, o erro entre a previsão e o valor real é calculado. O algoritmo de retropropagação ajusta os pesos das conexões para minimizar esse erro, usando técnicas como o gradiente descendente.

5. **Treinamento**: O MLP é treinado em um conjunto de dados usando o processo de retropropagação. Esse treinamento continua por várias iterações (épocas) até que o modelo alcance um desempenho satisfatório.

6. **Aplicações**: O MLP é amplamente utilizado em diversas aplicações, como reconhecimento de padrões, classificação, regressão e em problemas de previsão.

O MLP é uma das formas mais básicas de rede neural, mas sua estrutura permite resolver uma ampla variedade de problemas em aprendizado de máquina.

***

Uma rede neural feedforward é um tipo de rede neural onde as informações fluem em uma única direção: da camada de entrada para a camada de saída. Não há ciclos ou conexões que retrocedem, o que significa que os dados não são processados de volta em direção à entrada. 

Algumas características principais:

Estrutura: As redes feedforward são compostas por camadas de neurônios. Normalmente, têm pelo menos uma camada de entrada, uma ou mais camadas ocultas e uma camada de saída.

Propagação de Dados: Quando um conjunto de dados é fornecido, ele passa pela camada de entrada, é processado nas camadas ocultas e, por fim, chega à camada de saída, onde é gerada uma previsão ou classificação.

Sem Feedback: Como não há conexões que retornam à camada anterior, essas redes não têm memória de estados passados, o que as diferencia de redes neurais recorrentes (RNNs).

Funções de Ativação: Cada neurônio aplica uma função de ativação a sua saída, o que permite que a rede aprenda padrões complexos e não-lineares.

Treinamento: O treinamento é feito através do método de retropropagação, onde o erro entre a previsão da rede e o valor real é calculado e os pesos são ajustados para minimizar esse erro.

As redes feedforward são frequentemente utilizadas em tarefas como classificação de imagens, reconhecimento de voz e previsão de séries temporais, entre outras.

***

As funções de ativação desempenham um papel crucial nas redes neurais, pois introduzem não-linearidades que permitem que o modelo aprenda padrões complexos. Aqui estão algumas funções comuns:

### 1. **Sigmoid**
- **Definição**: A função sigmoid é dada por \( S(x) = \frac{1}{1 + e^{-x}} \).
- **Propriedades**:
  - Produz saídas no intervalo entre 0 e 1, tornando-a útil para problemas de classificação binária.
  - É suave e diferenciável em todos os pontos.
- **Limitações**:
  - Pode sofrer de "vanishing gradient" (gradientes que se tornam muito pequenos), o que pode dificultar o treinamento de redes mais profundas.
  - As saídas tendem a saturar (próximas de 0 ou 1) para grandes valores de entrada, levando a lentidão no aprendizado.

### 2. **ReLU (Rectified Linear Unit)**
- **Definição**: A função ReLU é definida como \( f(x) = \max(0, x) \).
- **Propriedades**:
  - Produz saídas zero para entradas negativas e linear para entradas positivas.
  - É computacionalmente eficiente e ajuda a mitigar o problema do "vanishing gradient".
- **Limitações**:
  - Pode sofrer de "dying ReLU", onde neurônios ficam inativos e nunca se ativam (saída constante de zero) para entradas negativas.

### 3. **Tanh (Tangente Hiperbólica)**
- **Definição**: A função tanh é dada por \( \text{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \).
- **Propriedades**:
  - Produz saídas no intervalo entre -1 e 1.
  - É uma função suave e diferenciável, semelhante à sigmoid, mas geralmente apresenta melhores resultados em termos de convergência.
- **Limitações**:
  - Também pode sofrer de "vanishing gradient", especialmente para entradas grandes.

### Por que essas funções introduzem não-linearidades?

1. **Representação de Dados Complexos**: Sem funções de ativação não-lineares, uma rede neural com múltiplas camadas seria equivalente a uma única camada linear. As combinações lineares de funções lineares resultam em funções lineares, limitando a capacidade do modelo de capturar padrões complexos.

2. **Flexibilidade**: A introdução de não-linearidades permite que a rede aprenda funções complexas, ajustando-se a diferentes distribuições de dados. Isso é crucial em problemas onde as relações entre as variáveis não são lineares.

3. **Treinamento**: A presença de não-linearidades permite que a retropropagação ajuste os pesos de maneira a otimizar a função de custo de forma eficaz, aprendendo representações mais úteis dos dados.

Essas funções ajudam a moldar o comportamento da rede, permitindo que ela aprenda representações mais complexas e adaptáveis dos dados de entrada.

***

Quantas camadas precisarão ser analisadas neste dataset?
Quais as funções que serão utilizadas para configurar a aprendizagem? (Pegar nos slides)
Qual é o processo que essas funções se aplicam e qual o resultado que elas geram?

Coletar os resultados e separá-los em apendices.